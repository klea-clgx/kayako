{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the data from the \"admin.jsonl\" file\n",
    "data = []\n",
    "with open(\"admin.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Find points without a \"cat\" label\n",
    "points_without_cat = []\n",
    "for entry in data:\n",
    "    if \"cats\" not in entry or not entry[\"cats\"]:\n",
    "        points_without_cat.append(entry)\n",
    "\n",
    "# Print the points without a \"cat\" label\n",
    "for entry in points_without_cat:\n",
    "    print(\"Point ID:\", entry[\"id\"])\n",
    "    print(\"Text:\", entry[\"text\"])\n",
    "    print(\"Entities:\", entry[\"entities\"])\n",
    "    print(\"Comments:\", entry.get(\"Comments\", []))  # Use .get() to handle missing \"Comments\" key\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data from the \"admin.jsonl\" file\n",
    "data = []\n",
    "with open(\"admin.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Separate features (email text and entities) and labels (cats) from the data\n",
    "features = [entry[\"text\"] for entry in data]\n",
    "entities = [entry[\"entities\"] for entry in data]\n",
    "labels = [entry[\"cats\"] for entry in data]\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "# First, split the data into training and the remaining data\n",
    "X_train, X_remaining, y_train, y_remaining, entities_train, entities_remaining = train_test_split(\n",
    "    features, labels, entities, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Then, split the remaining data into validation and test sets (equal split)\n",
    "X_val, X_test, y_val, y_test, entities_val, entities_test = train_test_split(\n",
    "    X_remaining, y_remaining, entities_remaining, test_size=0.5, random_state=42, stratify=y_remaining\n",
    ")\n",
    "\n",
    "# Now you have the following splits:\n",
    "# X_train, y_train, entities_train: Training set (80%)\n",
    "# X_val, y_val, entities_val: Validation set (10%)\n",
    "# X_test, y_test, entities_test: Test set (10%)\n",
    "\n",
    "# Optionally, you can save these splits into separate files for future use\n",
    "# For example:\n",
    "with open(\"train.jsonl\", \"w\") as file:\n",
    "    for text, cats, entities in zip(X_train, y_train, entities_train):\n",
    "        entry = {\n",
    "            \"text\": text,\n",
    "            \"cats\": cats,\n",
    "            \"entities\": entities\n",
    "        }\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "with open(\"val.jsonl\", \"w\") as file:\n",
    "    for text, cats, entities in zip(X_val, y_val, entities_val):\n",
    "        entry = {\n",
    "            \"text\": text,\n",
    "            \"cats\": cats,\n",
    "            \"entities\": entities\n",
    "        }\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "with open(\"test.jsonl\", \"w\") as file:\n",
    "    for text, cats, entities in zip(X_test, y_test, entities_test):\n",
    "        entry = {\n",
    "            \"text\": text,\n",
    "            \"cats\": cats,\n",
    "            \"entities\": entities\n",
    "        }\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      " report_request       0.81      0.57      0.67        53\n",
      "email_list_edit       1.00      0.21      0.35        28\n",
      "   report_issue       0.46      0.88      0.60        69\n",
      "  report_cancel       1.00      0.25      0.40        12\n",
      "          other       0.71      0.39      0.51        38\n",
      "\n",
      "       accuracy                           0.57       200\n",
      "      macro avg       0.80      0.46      0.51       200\n",
      "   weighted avg       0.71      0.57      0.56       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the data from the \"admin.jsonl\" file\n",
    "data = []\n",
    "with open(\"admin.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Extract text, entities, and categories from the data\n",
    "texts = [entry[\"text\"] for entry in data]\n",
    "entities = [entry[\"entities\"] for entry in data]\n",
    "categories = [entry[\"cats\"][0] if entry[\"cats\"] else \"other\" for entry in data]\n",
    "\n",
    "# Extract labeled entities from the text using spaCy NER\n",
    "extracted_entities = []\n",
    "for text, ents in zip(texts, entities):\n",
    "    doc = nlp(text)\n",
    "    extracted_entities.append([ent.label_ for ent in doc.ents])\n",
    "\n",
    "# Combine extracted entities with the original text\n",
    "combined_texts = [\" \".join([text] + entities) for text, entities in zip(texts, extracted_entities)]\n",
    "\n",
    "# Encode categories into numerical values\n",
    "category_mapping = {\n",
    "    \"report_request\": 0,\n",
    "    \"email_list_edit\": 1,\n",
    "    \"report_issue\": 2,\n",
    "    \"report_cancel\": 3,\n",
    "    \"other\": 4\n",
    "}\n",
    "labels = [category_mapping[cat] for cat in categories]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, y_pred, target_names=category_mapping.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      " report_request       0.89      0.64      0.75        53\n",
      "email_list_edit       0.90      0.32      0.47        28\n",
      "   report_issue       0.54      0.96      0.69        69\n",
      "  report_cancel       1.00      0.33      0.50        12\n",
      "          other       0.81      0.55      0.66        38\n",
      "\n",
      "       accuracy                           0.67       200\n",
      "      macro avg       0.83      0.56      0.61       200\n",
      "   weighted avg       0.76      0.67      0.66       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logistic_regression_model.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load data from admin.jsonl\n",
    "data = []\n",
    "with open(\"admin.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line)\n",
    "        text = entry[\"text\"]\n",
    "        entities = entry.get(\"entities\", [])\n",
    "        category = entry[\"cats\"][0] if entry[\"cats\"] else \"other\"\n",
    "\n",
    "        # Extract entities and combine them with the original text\n",
    "        entities_text = \" \".join([text[s:e] for s, e, _ in entities])\n",
    "        combined_text = text + \" \" + entities_text\n",
    "\n",
    "        data.append({\"text\": combined_text, \"category\": category})\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the category mapping\n",
    "category_mapping = {\n",
    "    \"report_request\": 0,\n",
    "    \"email_list_edit\": 1,\n",
    "    \"report_issue\": 2,\n",
    "    \"report_cancel\": 3,\n",
    "    \"other\": 4\n",
    "}\n",
    "\n",
    "# Map the categories to integers\n",
    "df[\"category\"] = df[\"category\"].map(category_mapping)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"category\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the combined texts using TF-IDF or CountVectorizer\n",
    "vectorizer = TfidfVectorizer()  # You can also use CountVectorizer if you prefer\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, y_pred, target_names=category_mapping.keys()))\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: Hi Mercury, Please send me a list of appraisers that cover the following counties in Kansas. Jackson Johnson Clay Platt Buchanan I need appraiser names, addresses and tier ratings. Let me know if you need more information. Thanks! Tracie Draper (Mortgage) Appraisal Desk, Lead Email: tracie.draper@primelending.com Website: primelending.com Phone: (972) 738-7739 PrimeLending NMLS#: 13649. Equal Housing Lender 18111 Preston Road, Suite 900, Dallas, TX 75252 \t\t \t\t \t\t\t\t ________________________________ PrimeLending, A PlainsCapital Company NMLS # 13649, Equal Housing Lender. \n",
      "Predicted Entities: [('client',)]\n",
      "Predicted Email Category: ['report_request']\n",
      "Classification Report for Entity Classification Model:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      add_emails       0.89      0.35      0.50        23\n",
      "      add_fields       0.93      0.39      0.55        33\n",
      "       attention       0.00      0.00      0.00         0\n",
      "   change_fields       0.00      0.00      0.00         1\n",
      "          client       0.92      0.83      0.87       110\n",
      "   create_report       0.90      0.42      0.58        45\n",
      "   emails_to_add       0.80      0.24      0.36        17\n",
      "emails_to_remove       0.00      0.00      0.00         7\n",
      "   fields_to_add       1.00      0.44      0.61        25\n",
      "fields_to_change       0.00      0.00      0.00         0\n",
      "fields_to_remove       0.00      0.00      0.00         1\n",
      "  issue_to_check       0.82      0.93      0.87        96\n",
      "   remove_emails       1.00      0.16      0.27        19\n",
      "   remove_fields       0.00      0.00      0.00         1\n",
      "   remove_report       1.00      0.25      0.40        12\n",
      "          urgent       1.00      0.04      0.08        25\n",
      "\n",
      "       micro avg       0.88      0.58      0.70       415\n",
      "       macro avg       0.58      0.25      0.32       415\n",
      "    weighted avg       0.88      0.58      0.65       415\n",
      "     samples avg       0.71      0.55      0.60       415\n",
      "\n",
      "Classification Report for Email Category Classification Model:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "email_list_edit       0.94      0.70      0.81       115\n",
      "          other       0.91      0.80      0.85       194\n",
      "  report_cancel       1.00      0.55      0.71        64\n",
      "   report_issue       0.76      0.95      0.85       380\n",
      " report_request       0.91      0.85      0.88       247\n",
      "\n",
      "       accuracy                           0.84      1000\n",
      "      macro avg       0.90      0.77      0.82      1000\n",
      "   weighted avg       0.86      0.84      0.84      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaydenlea/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kaydenlea/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaydenlea/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaydenlea/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaydenlea/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the data from admin.jsonl\n",
    "data = []\n",
    "with open(\"admin.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line)\n",
    "        data.append(entry)\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract the entity labels\n",
    "def extract_entity_labels(entities):\n",
    "    return [entity[2] for entity in entities]\n",
    "\n",
    "df['entity_labels'] = df['entities'].apply(extract_entity_labels)\n",
    "\n",
    "# Combine the identified entities with the original text\n",
    "df['combined_text'] = df.apply(lambda row: \" \".join([row['text'][start:end] for start, end, _ in row['entities']]), axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"combined_text\"], df[\"entity_labels\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Entity Classification Model\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_transformed = mlb.fit_transform(y_train)\n",
    "y_test_transformed = mlb.transform(y_test)\n",
    "\n",
    "base_classifier = LogisticRegression(max_iter=1000)\n",
    "multi_label_classifier = MultiOutputClassifier(base_classifier)\n",
    "multi_label_classifier.fit(X_train_vectorized, y_train_transformed)\n",
    "\n",
    "# Convert the \"cats\" field into a flat list of category labels\n",
    "category_labels = [label for sublist in df[\"cats\"] for label in sublist]\n",
    "\n",
    "# Train the Email Category Classification Model\n",
    "category_vectorizer = TfidfVectorizer()\n",
    "X_train_category_vectorized = category_vectorizer.fit_transform(df[\"text\"])\n",
    "category_encoder = LabelEncoder()\n",
    "y_train_category_encoded = category_encoder.fit_transform(category_labels)\n",
    "\n",
    "category_model = LogisticRegression(max_iter=1000)\n",
    "category_model.fit(X_train_category_vectorized, y_train_category_encoded)\n",
    "\n",
    "# Save the trained models to files\n",
    "joblib.dump(vectorizer, \"entity_vectorizer.pkl\")\n",
    "joblib.dump(multi_label_classifier, \"entity_classification_model.pkl\")\n",
    "joblib.dump(category_vectorizer, \"category_vectorizer.pkl\")\n",
    "joblib.dump(category_encoder, \"category_encoder.pkl\")\n",
    "joblib.dump(category_model, \"category_classification_model.pkl\")\n",
    "\n",
    "# Sample text to test the model\n",
    "sample_text = \"Hi Mercury, Please send me a list of appraisers that cover the following counties in Kansas. Jackson Johnson Clay Platt Buchanan I need appraiser names, addresses and tier ratings. Let me know if you need more information. Thanks! Tracie Draper (Mortgage) Appraisal Desk, Lead Email: tracie.draper@primelending.com Website: primelending.com Phone: (972) 738-7739 PrimeLending NMLS#: 13649. Equal Housing Lender 18111 Preston Road, Suite 900, Dallas, TX 75252 \\t\\t \\t\\t \\t\\t\\t\\t ________________________________ PrimeLending, A PlainsCapital Company NMLS # 13649, Equal Housing Lender. \"\n",
    "\n",
    "# Use the entity classification model to predict entities in the sample text\n",
    "sample_text_vectorized = vectorizer.transform([sample_text])\n",
    "predicted_labels = multi_label_classifier.predict(sample_text_vectorized)\n",
    "predicted_entities = mlb.inverse_transform(predicted_labels)\n",
    "\n",
    "# Use the predicted entities to predict the email category\n",
    "sample_text_category_vectorized = category_vectorizer.transform([sample_text])\n",
    "predicted_category_encoded = category_model.predict(sample_text_category_vectorized)\n",
    "predicted_category_encoded = predicted_category_encoded.reshape(-1, 1)\n",
    "predicted_category = category_encoder.inverse_transform(predicted_category_encoded)\n",
    "\n",
    "print(\"Sample Text:\", sample_text)\n",
    "print(\"Predicted Entities:\", predicted_entities)\n",
    "print(\"Predicted Email Category:\", predicted_category)\n",
    "\n",
    "# Classification Report for Entity Classification Model\n",
    "y_pred_entities = multi_label_classifier.predict(X_test_vectorized)\n",
    "target_names_entities = mlb.classes_\n",
    "print(\"Classification Report for Entity Classification Model:\")\n",
    "print(classification_report(y_test_transformed, y_pred_entities, target_names=target_names_entities))\n",
    "\n",
    "# Classification Report for Email Category Classification Model\n",
    "y_pred_category = category_model.predict(X_train_category_vectorized)\n",
    "target_names_category = category_encoder.classes_\n",
    "print(\"Classification Report for Email Category Classification Model:\")\n",
    "print(classification_report(y_train_category_encoded, y_pred_category, target_names=target_names_category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the data from admin.jsonl\n",
    "data = []\n",
    "with open(\"admin.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Create a list to store the extracted entities\n",
    "entities_data = []\n",
    "\n",
    "# Extract the entities and their corresponding entity labels\n",
    "for _, row in enumerate(data):\n",
    "    text = row[\"text\"]\n",
    "    entities = row[\"entities\"]\n",
    "    for start, end, entity_label in entities:\n",
    "        entity_text = text[start:end]\n",
    "        entities_data.append({\"text\": entity_text, \"category\": entity_label})\n",
    "\n",
    "# Save the entity data to a separate file (e.g., entities_data.jsonl)\n",
    "with open(\"entities_data.jsonl\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for entity in entities_data:\n",
    "        output_file.write(json.dumps(entity, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: I would like to request a new report\n",
      "Entity: create_report\n",
      "Character Range: 24-25 (Character: a)\n",
      "Character Range: 30-36 (Character: report)\n",
      "Entity: issue_to_check\n",
      "Character Range: 13-15 (Character: to)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "\n",
    "# Load the data from admin.jsonl\n",
    "data = []\n",
    "with open(\"admin.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line)\n",
    "        data.append(entry)\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract the entity labels\n",
    "def extract_entity_labels(entities):\n",
    "    return [entity[2] for entity in entities]\n",
    "\n",
    "df['entity_labels'] = df['entities'].apply(extract_entity_labels)\n",
    "\n",
    "# Combine the identified entities with the original text\n",
    "df['combined_text'] = df.apply(lambda row: \" \".join([row['text'][start:end] for start, end, _ in row['entities']]), axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"combined_text\"], df[\"entity_labels\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Entity Classification Model\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Use MultiLabelBinarizer to convert the entity labels to binary arrays\n",
    "binarizer = MultiLabelBinarizer()\n",
    "y_train_binary = binarizer.fit_transform(y_train)\n",
    "y_test_binary = binarizer.transform(y_test)\n",
    "\n",
    "entity_classifier = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "entity_classifier.fit(X_train_vectorized, y_train_binary)\n",
    "\n",
    "# Save the trained model and vectorizer to separate files with different names\n",
    "joblib.dump(vectorizer, \"entity_vectorizer_admin.pkl\")\n",
    "joblib.dump(entity_classifier, \"entity_classification_model_admin.pkl\")\n",
    "\n",
    "# Sample text to test the model\n",
    "sample_text = \"I would like to request a new report\"\n",
    "# Use the entity classification model to predict entities in the sample text\n",
    "sample_text_vectorized = vectorizer.transform([sample_text])\n",
    "predicted_labels = entity_classifier.predict(sample_text_vectorized)\n",
    "predicted_entities = binarizer.inverse_transform(predicted_labels)\n",
    "\n",
    "# Function to get character ranges for each entity\n",
    "\n",
    "def get_character_ranges(text, entity):\n",
    "    ranges = []\n",
    "    start_idx = 0\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        end_idx = start_idx + len(word)\n",
    "        if word in entity:\n",
    "            ranges.append((start_idx, end_idx, word))\n",
    "        start_idx = end_idx + 1\n",
    "    return ranges\n",
    "\n",
    "\n",
    "# Use the predicted entities to get character ranges in the sample text\n",
    "entities_with_ranges = []\n",
    "for entity in predicted_entities[0]:\n",
    "    ranges = get_character_ranges(sample_text, entity)\n",
    "    entities_with_ranges.append((entity, ranges))\n",
    "\n",
    "# Print the results\n",
    "print(\"Sample Text:\", sample_text)\n",
    "for entity, ranges in entities_with_ranges:\n",
    "    print(\"Entity:\", entity)\n",
    "    if ranges:\n",
    "        for start, end, word in ranges:\n",
    "            print(f\"Character Range: {start}-{end} (Character: {word})\")\n",
    "    else:\n",
    "        print(\"Character Range: Not Found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: I woul dlike to request a new report\n",
      "Entity: create_report\n",
      "Character Range: 24-25 (Character: a)\n",
      "Character Range: 30-36 (Character: report)\n",
      "Predicted Email Category: other\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Load the data from admin.jsonl\n",
    "data = []\n",
    "with open(\"admin.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line)\n",
    "        data.append(entry)\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract the entity labels\n",
    "def extract_entity_labels(entities):\n",
    "    return [entity[2] for entity in entities]\n",
    "\n",
    "df['entity_labels'] = df['entities'].apply(extract_entity_labels)\n",
    "\n",
    "# Combine the identified entities with the original text\n",
    "df['combined_text'] = df.apply(lambda row: \" \".join([row['text'][start:end] for start, end, _ in row['entities']]), axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"combined_text\"], df[\"entity_labels\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Entity Classification Model\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Use MultiLabelBinarizer to convert the entity labels to binary arrays\n",
    "binarizer = MultiLabelBinarizer()\n",
    "y_train_binary = binarizer.fit_transform(y_train)\n",
    "y_test_binary = binarizer.transform(y_test)\n",
    "\n",
    "entity_classifier = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "entity_classifier.fit(X_train_vectorized, y_train_binary)\n",
    "\n",
    "# Save the trained model and vectorizer to separate files with different names\n",
    "joblib.dump(vectorizer, \"entity_vectorizer_admin.pkl\")\n",
    "joblib.dump(entity_classifier, \"entity_classification_model_admin.pkl\")\n",
    "\n",
    "# Load the pre-trained email category classification model and vectorizer\n",
    "category_vectorizer = joblib.load(\"category_vectorizer.pkl\")\n",
    "category_model = joblib.load(\"category_classification_model.pkl\")\n",
    "category_encoder = LabelEncoder()\n",
    "\n",
    "# Sample text to test the model\n",
    "sample_text = \"I woul dlike to request a new report\"\n",
    "\n",
    "# Use the entity classification model to predict entities in the sample text\n",
    "sample_text_vectorized = vectorizer.transform([sample_text])\n",
    "predicted_labels = entity_classifier.predict(sample_text_vectorized)\n",
    "predicted_entities = binarizer.inverse_transform(predicted_labels)\n",
    "\n",
    "# Function to get character ranges for each entity\n",
    "def get_character_ranges(text, entity):\n",
    "    ranges = []\n",
    "    start_idx = 0\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        end_idx = start_idx + len(word)\n",
    "        if word in entity:\n",
    "            ranges.append((start_idx, end_idx, word))\n",
    "        start_idx = end_idx + 1\n",
    "    return ranges\n",
    "\n",
    "# Use the predicted entities to get character ranges in the sample text\n",
    "entities_with_ranges = []\n",
    "for entity in predicted_entities[0]:\n",
    "    ranges = get_character_ranges(sample_text, entity)\n",
    "    entities_with_ranges.append((entity, ranges))\n",
    "\n",
    "\n",
    "# Use the predicted entities to predict the email category\n",
    "sample_text_category_vectorized = category_vectorizer.transform([sample_text])\n",
    "predicted_category_encoded = category_model.predict(sample_text_category_vectorized)\n",
    "\n",
    "# Map the numerical category predictions to human-readable text using category_mapping\n",
    "category_mapping = {\n",
    "    0: \"report_request\",\n",
    "    1: \"email_list_edit\",\n",
    "    2: \"report_issue\",\n",
    "    3: \"report_cancel\",\n",
    "    4: \"other\"\n",
    "}\n",
    "predicted_category_text = [category_mapping[category] for category in predicted_category_encoded]\n",
    "\n",
    "# Print the results\n",
    "print(\"Sample Text:\", sample_text)\n",
    "for entity, ranges in entities_with_ranges:\n",
    "    print(\"Entity:\", entity)\n",
    "    if ranges:\n",
    "        for start, end, word in ranges:\n",
    "            print(f\"Character Range: {start}-{end} (Character: {word})\")\n",
    "    else:\n",
    "        print(\"Character Range: Not Found\")\n",
    "\n",
    "print(\"Predicted Email Category:\", predicted_category_text[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
